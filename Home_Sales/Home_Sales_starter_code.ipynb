{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_KW73O2e3dw",
    "outputId": "fa5fd2b3-e2de-491b-ee1c-405317ba7ebc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import findspark and initialize.\n",
    "import pandas as pd\n",
    "import os\n",
    "import findspark\n",
    "import time\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import round, avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2XbWNf1Te5fM"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOJqxG_RPSwp",
    "outputId": "7857ef9f-5b04-405d-f5aa-e535dfe7870c"
   },
   "outputs": [],
   "source": [
    "# 1. Read the home_sales_revised.csv from the provided AWS S3 bucket location into a PySpark DataFrame.\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RoljcJ7WPpnm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|                  id|      date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923|       4|        3|       3167|   11733|     2|         1|  76|\n",
      "|7530a2d8-1ae3-451...|2021-06-13|      2013|379628|       2|        2|       2235|   14384|     1|         0|  23|\n",
      "|43de979c-0bf0-4c9...|2019-04-12|      2014|417866|       2|        2|       2127|   10575|     2|         0|   0|\n",
      "|b672c137-b88c-48b...|2019-10-16|      2016|239895|       2|        2|       1631|   11149|     2|         0|   0|\n",
      "|e0726d4d-d595-407...|2022-01-08|      2017|424418|       3|        2|       2249|   13878|     2|         0|   4|\n",
      "|5aa00529-0533-46b...|2019-01-30|      2017|218712|       2|        3|       1965|   14375|     2|         0|   7|\n",
      "|131492a1-72e2-4a8...|2020-02-08|      2017|419199|       2|        3|       2062|    8876|     2|         0|   6|\n",
      "|8d54a71b-c520-44e...|2019-07-21|      2010|323956|       2|        3|       1506|   11816|     1|         0|  25|\n",
      "|e81aacfe-17fe-46b...|2020-06-16|      2016|181925|       3|        3|       2137|   11709|     2|         0|  22|\n",
      "|2ed8d509-7372-46d...|2021-08-06|      2015|258710|       3|        3|       1918|    9666|     1|         0|  25|\n",
      "|f876d86f-3c9f-42b...|2019-02-27|      2011|167864|       3|        3|       2471|   13924|     2|         0|  15|\n",
      "|0a2bd445-8508-4d8...|2021-12-30|      2014|337527|       2|        3|       1926|   12556|     1|         0|  23|\n",
      "|941bad30-eb49-4a7...|2020-05-09|      2015|229896|       3|        3|       2197|    8641|     1|         0|   3|\n",
      "|dd61eb34-6589-4c0...|2021-07-25|      2016|210247|       3|        2|       1672|   11986|     2|         0|  28|\n",
      "|f1e4cef7-d151-439...|2019-02-01|      2011|398667|       2|        3|       2331|   11356|     1|         0|   7|\n",
      "|ea620c7b-c2f7-4c6...|2021-05-31|      2011|437958|       3|        3|       2356|   11052|     1|         0|  26|\n",
      "|f233cb41-6f33-4b0...|2021-07-18|      2016|437375|       4|        3|       1704|   11721|     2|         0|  34|\n",
      "|c797ca12-52cd-4b1...|2019-06-08|      2015|288650|       2|        3|       2100|   10419|     2|         0|   7|\n",
      "|0cfe57f3-28c2-472...|2019-10-04|      2015|308313|       3|        3|       1960|    9453|     2|         0|   2|\n",
      "|4566cd2a-ac6e-435...|2019-07-15|      2016|177541|       3|        3|       2130|   10517|     2|         0|  25|\n",
      "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Create a temporary view of the DataFrame.\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33287"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- date_built: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- bedrooms: string (nullable = true)\n",
      " |-- bathrooms: string (nullable = true)\n",
      " |-- sqft_living: string (nullable = true)\n",
      " |-- sqft_lot: string (nullable = true)\n",
      " |-- floors: string (nullable = true)\n",
      " |-- waterfront: string (nullable = true)\n",
      " |-- view: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the data types of the columns. \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, avg, round, col\n",
    "\n",
    "# Ensure correct data types\n",
    "df = df.withColumn(\"sale_date\", col(\"date\").cast(\"date\")) \\\n",
    "       .withColumn(\"price\", col(\"price\").cast(\"double\")) \\\n",
    "       .withColumn(\"bedrooms\", col(\"bedrooms\").cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- date_built: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- bathrooms: string (nullable = true)\n",
      " |-- sqft_living: string (nullable = true)\n",
      " |-- sqft_lot: string (nullable = true)\n",
      " |-- floors: string (nullable = true)\n",
      " |-- waterfront: string (nullable = true)\n",
      " |-- view: string (nullable = true)\n",
      " |-- sale_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6fkwOeOmqvq",
    "outputId": "bdded620-79c4-488d-c7a5-91c6799c419e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|sale_year|avg_price|\n",
      "+---------+---------+\n",
      "|     2019| 300263.7|\n",
      "|     2020|298353.78|\n",
      "|     2021|301819.44|\n",
      "|     2022|296363.88|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. What is the average price for a four bedroom house sold per year, rounded to two decimal places?\n",
    "\n",
    "# Filter for four-bedroom houses\n",
    "four_bed_df = df.filter(col(\"bedrooms\") == 4)\n",
    "\n",
    "# Extract sale year and calculate average price per year\n",
    "avg_price_per_year = (\n",
    "    four_bed_df.withColumn(\"sale_year\", year(col(\"sale_date\")))\n",
    "    .groupBy(\"sale_year\")\n",
    "    .agg(round(avg(\"price\"), 2).alias(\"avg_price\"))\n",
    "    .orderBy(\"sale_year\")\n",
    ")\n",
    "\n",
    "# Show results\n",
    "avg_price_per_year.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|year|avg_price|\n",
      "+----+---------+\n",
      "|2022|296363.88|\n",
      "|2021|301819.44|\n",
      "|2020|298353.78|\n",
      "|2019| 300263.7|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.2 What is the average price for a four bedroom house sold per year, rounded to two decimal places?\n",
    "\n",
    "# Ensure proper data types\n",
    "df = df.withColumn(\"sale_date\", col(\"date\").cast(\"date\")) \\\n",
    "       .withColumn(\"price\", col(\"price\").cast(\"double\")) \\\n",
    "       .withColumn(\"bedrooms\", col(\"bedrooms\").cast(\"int\"))\n",
    "\n",
    "# Register DataFrame as a SQL temporary table\n",
    "df.createOrReplaceTempView(\"home_data\")\n",
    "\n",
    "# Run SQL query\n",
    "avg_price_4_bedroom = spark.sql(\"\"\"\n",
    "    SELECT YEAR(sale_date) AS year, ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM home_data\n",
    "    WHERE bedrooms = 4\n",
    "    GROUP BY YEAR(sale_date)\n",
    "    ORDER BY YEAR(sale_date) DESC\n",
    "\"\"\")\n",
    "\n",
    "# Show results\n",
    "avg_price_4_bedroom.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8p_tUS8h8it",
    "outputId": "65806e5f-6262-41c0-ff65-5107464e5c4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|date_built|avg_price|\n",
      "+----------+---------+\n",
      "|      2017|292676.79|\n",
      "|      2016|290555.07|\n",
      "|      2015| 288770.3|\n",
      "|      2014|290852.27|\n",
      "|      2013|295962.27|\n",
      "|      2012|293683.19|\n",
      "|      2011|291117.47|\n",
      "|      2010|292859.62|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. What is the average price of a home for each year the home was built,\n",
    "# that have 3 bedrooms and 3 bathrooms, rounded to two decimal places?\n",
    "# Ensure correct data types\n",
    "df = df.withColumn(\"date_built\", col(\"date_built\").cast(\"int\"))  # Ensure date_built is numeric\n",
    "\n",
    "# Register DataFrame as a SQL temporary table\n",
    "df.createOrReplaceTempView(\"home_data\")\n",
    "\n",
    "# Run SQL query\n",
    "avg_price_3_bed_3_bath = spark.sql(\"\"\"\n",
    "    SELECT date_built, ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM home_data\n",
    "    WHERE bedrooms = 3 AND bathrooms = 3\n",
    "    GROUP BY date_built\n",
    "    ORDER BY date_built DESC\n",
    "\"\"\")\n",
    "\n",
    "# Show results\n",
    "avg_price_3_bed_3_bath.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-Eytz64liDU",
    "outputId": "17119810-56ad-40c3-de5e-c3db57e43bcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|date_built|avg_price |\n",
      "+----------+----------+\n",
      "|2017      |280,317.58|\n",
      "|2016      |293,965.10|\n",
      "|2015      |297,609.97|\n",
      "|2014      |298,264.72|\n",
      "|2013      |303,676.79|\n",
      "|2012      |307,539.97|\n",
      "|2011      |276,553.81|\n",
      "|2010      |285,010.22|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. What is the average price of a home for each year the home was built,\n",
    "# that have 3 bedrooms, 3 bathrooms, with two floors,\n",
    "# and are greater than or equal to 2,000 square feet, rounded to two decimal places?\n",
    "\n",
    "# Ensure correct data types\n",
    "df = df.withColumn(\"floors\", col(\"floors\").cast(\"int\")) \\\n",
    "       .withColumn(\"sqft_living\", col(\"sqft_living\").cast(\"int\")) \\\n",
    "\n",
    "# Register DataFrame as a SQL temporary table\n",
    "df.createOrReplaceTempView(\"home_data\")\n",
    "\n",
    "# Run SQL query\n",
    "avg_price_filtered = spark.sql(\"\"\"\n",
    "    SELECT date_built, \n",
    "           format_number(ROUND(AVG(price), 2), 2) AS avg_price\n",
    "    FROM home_data\n",
    "    WHERE bedrooms = 3 \n",
    "    AND bathrooms = 3\n",
    "    AND floors = 2\n",
    "    AND sqft_living >= 2000\n",
    "    GROUP BY date_built\n",
    "    ORDER BY date_built DESC\n",
    "\"\"\")\n",
    "\n",
    "avg_price_filtered.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUrfgOX1pCRd",
    "outputId": "17c25774-855e-4290-a4bd-a04902bdc13a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|view| avg_price|\n",
      "+----+----------+\n",
      "| 100| 1026669.5|\n",
      "|  99|1061201.42|\n",
      "|  98|1053739.33|\n",
      "|  97|1129040.15|\n",
      "|  96|1017815.92|\n",
      "|  95| 1054325.6|\n",
      "|  94| 1033536.2|\n",
      "|  93|1026006.06|\n",
      "|  92| 970402.55|\n",
      "|  91|1137372.73|\n",
      "|  90|1062654.16|\n",
      "|  89|1107839.15|\n",
      "|  88|1031719.35|\n",
      "|  87| 1072285.2|\n",
      "|  86|1070444.25|\n",
      "|  85|1056336.74|\n",
      "|  84|1117233.13|\n",
      "|  83|1033965.93|\n",
      "|  82| 1063498.0|\n",
      "|  81|1053472.79|\n",
      "+----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Query run time: 4.4938 seconds\n"
     ]
    }
   ],
   "source": [
    "# 6. What is the average price of a home per \"view\" rating, rounded to two decimal places,\n",
    "# having an average home price greater than or equal to $350,000? Order by descending view rating.\n",
    "# Although this is a small dataset, determine the run time for this query.\n",
    "\n",
    "\n",
    "\n",
    "# Ensure correct data types\n",
    "df = df.withColumn(\"view\", col(\"view\").cast(\"int\")) \n",
    "\n",
    "# Register DataFrame as a SQL temporary table\n",
    "df.createOrReplaceTempView(\"home_data\")\n",
    "\n",
    "# Measure query execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Run SQL query\n",
    "avg_price_filtered = spark.sql(\"\"\"\n",
    "    SELECT view, ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM home_data\n",
    "    GROUP BY view\n",
    "    HAVING avg_price >= 350000\n",
    "    ORDER BY view DESC\n",
    "\"\"\")\n",
    "\n",
    "# Show results\n",
    "avg_price_filtered.show()\n",
    "\n",
    "# Calculate run time\n",
    "end_time = time.time()\n",
    "print(f\"Query run time: {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAhk3ZD2tFy8",
    "outputId": "0a8f132d-40a8-4bd4-b5f2-2847e98427f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Cache the the temporary table home_sales.\n",
    "# Cache the table using SQL\n",
    "spark.sql(\"cache table home_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4opVhbvxtL-i",
    "outputId": "38ec8487-795f-4550-b50c-fcc6f2b7c769"
   },
   "outputs": [],
   "source": [
    "# 8. Check if the table is cached.\n",
    "# Check if the table is cached\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+--------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|                  id|      date|date_built|   price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view| sale_date|\n",
      "+--------------------+----------+----------+--------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923.0|       4|        3|       3167|   11733|     2|         1|  76|2022-04-08|\n",
      "|7530a2d8-1ae3-451...|2021-06-13|      2013|379628.0|       2|        2|       2235|   14384|     1|         0|  23|2021-06-13|\n",
      "|43de979c-0bf0-4c9...|2019-04-12|      2014|417866.0|       2|        2|       2127|   10575|     2|         0|   0|2019-04-12|\n",
      "|b672c137-b88c-48b...|2019-10-16|      2016|239895.0|       2|        2|       1631|   11149|     2|         0|   0|2019-10-16|\n",
      "|e0726d4d-d595-407...|2022-01-08|      2017|424418.0|       3|        2|       2249|   13878|     2|         0|   4|2022-01-08|\n",
      "|5aa00529-0533-46b...|2019-01-30|      2017|218712.0|       2|        3|       1965|   14375|     2|         0|   7|2019-01-30|\n",
      "|131492a1-72e2-4a8...|2020-02-08|      2017|419199.0|       2|        3|       2062|    8876|     2|         0|   6|2020-02-08|\n",
      "|8d54a71b-c520-44e...|2019-07-21|      2010|323956.0|       2|        3|       1506|   11816|     1|         0|  25|2019-07-21|\n",
      "|e81aacfe-17fe-46b...|2020-06-16|      2016|181925.0|       3|        3|       2137|   11709|     2|         0|  22|2020-06-16|\n",
      "|2ed8d509-7372-46d...|2021-08-06|      2015|258710.0|       3|        3|       1918|    9666|     1|         0|  25|2021-08-06|\n",
      "|f876d86f-3c9f-42b...|2019-02-27|      2011|167864.0|       3|        3|       2471|   13924|     2|         0|  15|2019-02-27|\n",
      "|0a2bd445-8508-4d8...|2021-12-30|      2014|337527.0|       2|        3|       1926|   12556|     1|         0|  23|2021-12-30|\n",
      "|941bad30-eb49-4a7...|2020-05-09|      2015|229896.0|       3|        3|       2197|    8641|     1|         0|   3|2020-05-09|\n",
      "|dd61eb34-6589-4c0...|2021-07-25|      2016|210247.0|       3|        2|       1672|   11986|     2|         0|  28|2021-07-25|\n",
      "|f1e4cef7-d151-439...|2019-02-01|      2011|398667.0|       2|        3|       2331|   11356|     1|         0|   7|2019-02-01|\n",
      "|ea620c7b-c2f7-4c6...|2021-05-31|      2011|437958.0|       3|        3|       2356|   11052|     1|         0|  26|2021-05-31|\n",
      "|f233cb41-6f33-4b0...|2021-07-18|      2016|437375.0|       4|        3|       1704|   11721|     2|         0|  34|2021-07-18|\n",
      "|c797ca12-52cd-4b1...|2019-06-08|      2015|288650.0|       2|        3|       2100|   10419|     2|         0|   7|2019-06-08|\n",
      "|0cfe57f3-28c2-472...|2019-10-04|      2015|308313.0|       3|        3|       1960|    9453|     2|         0|   2|2019-10-04|\n",
      "|4566cd2a-ac6e-435...|2019-07-15|      2016|177541.0|       3|        3|       2130|   10517|     2|         0|  25|2019-07-15|\n",
      "+--------------------+----------+----------+--------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"home_sales\")\n",
    "df.show()  # Check if the DataFrame has been loaded properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='home_data', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='home_sales', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"home_sales\")\n",
    "spark.catalog.listTables()  # List all tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+--------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|                  id|      date|date_built|   price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view| sale_date|\n",
      "+--------------------+----------+----------+--------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923.0|       4|        3|       3167|   11733|     2|         1|  76|2022-04-08|\n",
      "|7530a2d8-1ae3-451...|2021-06-13|      2013|379628.0|       2|        2|       2235|   14384|     1|         0|  23|2021-06-13|\n",
      "|43de979c-0bf0-4c9...|2019-04-12|      2014|417866.0|       2|        2|       2127|   10575|     2|         0|   0|2019-04-12|\n",
      "|b672c137-b88c-48b...|2019-10-16|      2016|239895.0|       2|        2|       1631|   11149|     2|         0|   0|2019-10-16|\n",
      "|e0726d4d-d595-407...|2022-01-08|      2017|424418.0|       3|        2|       2249|   13878|     2|         0|   4|2022-01-08|\n",
      "|5aa00529-0533-46b...|2019-01-30|      2017|218712.0|       2|        3|       1965|   14375|     2|         0|   7|2019-01-30|\n",
      "|131492a1-72e2-4a8...|2020-02-08|      2017|419199.0|       2|        3|       2062|    8876|     2|         0|   6|2020-02-08|\n",
      "|8d54a71b-c520-44e...|2019-07-21|      2010|323956.0|       2|        3|       1506|   11816|     1|         0|  25|2019-07-21|\n",
      "|e81aacfe-17fe-46b...|2020-06-16|      2016|181925.0|       3|        3|       2137|   11709|     2|         0|  22|2020-06-16|\n",
      "|2ed8d509-7372-46d...|2021-08-06|      2015|258710.0|       3|        3|       1918|    9666|     1|         0|  25|2021-08-06|\n",
      "|f876d86f-3c9f-42b...|2019-02-27|      2011|167864.0|       3|        3|       2471|   13924|     2|         0|  15|2019-02-27|\n",
      "|0a2bd445-8508-4d8...|2021-12-30|      2014|337527.0|       2|        3|       1926|   12556|     1|         0|  23|2021-12-30|\n",
      "|941bad30-eb49-4a7...|2020-05-09|      2015|229896.0|       3|        3|       2197|    8641|     1|         0|   3|2020-05-09|\n",
      "|dd61eb34-6589-4c0...|2021-07-25|      2016|210247.0|       3|        2|       1672|   11986|     2|         0|  28|2021-07-25|\n",
      "|f1e4cef7-d151-439...|2019-02-01|      2011|398667.0|       2|        3|       2331|   11356|     1|         0|   7|2019-02-01|\n",
      "|ea620c7b-c2f7-4c6...|2021-05-31|      2011|437958.0|       3|        3|       2356|   11052|     1|         0|  26|2021-05-31|\n",
      "|f233cb41-6f33-4b0...|2021-07-18|      2016|437375.0|       4|        3|       1704|   11721|     2|         0|  34|2021-07-18|\n",
      "|c797ca12-52cd-4b1...|2019-06-08|      2015|288650.0|       2|        3|       2100|   10419|     2|         0|   7|2019-06-08|\n",
      "|0cfe57f3-28c2-472...|2019-10-04|      2015|308313.0|       3|        3|       1960|    9453|     2|         0|   2|2019-10-04|\n",
      "|4566cd2a-ac6e-435...|2019-07-15|      2016|177541.0|       3|        3|       2130|   10517|     2|         0|  25|2019-07-15|\n",
      "+--------------------+----------+----------+--------+--------+---------+-----------+--------+------+----------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create or replace the temporary view\n",
    "df.createOrReplaceTempView(\"home_sales\")\n",
    "\n",
    "# Now run your SQL query\n",
    "result = spark.sql(\"SELECT * FROM home_sales\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the 'home_sales' table cached? True\n"
     ]
    }
   ],
   "source": [
    "# Check if the 'home_sales' table is cached\n",
    "is_cached = spark.catalog.isCached(\"home_sales\")\n",
    "print(f\"Is the 'home_sales' table cached? {is_cached}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GnL46lwTSEk",
    "outputId": "09a16c73-194d-4371-95d1-ee64fe83b91c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|view| avg_price|\n",
      "+----+----------+\n",
      "| 100| 1026669.5|\n",
      "|  99|1061201.42|\n",
      "|  98|1053739.33|\n",
      "|  97|1129040.15|\n",
      "|  96|1017815.92|\n",
      "|  95| 1054325.6|\n",
      "|  94| 1033536.2|\n",
      "|  93|1026006.06|\n",
      "|  92| 970402.55|\n",
      "|  91|1137372.73|\n",
      "|  90|1062654.16|\n",
      "|  89|1107839.15|\n",
      "|  88|1031719.35|\n",
      "|  87| 1072285.2|\n",
      "|  86|1070444.25|\n",
      "|  85|1056336.74|\n",
      "|  84|1117233.13|\n",
      "|  83|1033965.93|\n",
      "|  82| 1063498.0|\n",
      "|  81|1053472.79|\n",
      "+----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "--- 1.4993374347686768 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# 9. Using the cached data, run the last query above, that calculates\n",
    "# the average price of a home per \"view\" rating, rounded to two decimal places,\n",
    "# having an average home price greater than or equal to $350,000.\n",
    "# Determine the runtime and compare it to the uncached runtime.\n",
    "\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute the query\n",
    "cached_result = spark.sql(\"\"\"\n",
    "    SELECT view, ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM home_sales\n",
    "    GROUP BY view\n",
    "    HAVING avg_price >= 350000\n",
    "    ORDER BY view DESC\n",
    "\"\"\")\n",
    "\n",
    "# Show results\n",
    "cached_result.show()\n",
    "\n",
    "# End timer and print execution time\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Qm12WN9isHBR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned Parquet file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# 10. Partition by the \"date_built\" field on the formatted parquet home sales data\n",
    "# Define the output path for the partitioned Parquet data\n",
    "output_path = \"home_sales_partitioned\"\n",
    "\n",
    "# Write the DataFrame as a partitioned Parquet file\n",
    "df.write.mode(\"overwrite\").partitionBy(\"date_built\").parquet(output_path)\n",
    "\n",
    "print(\"Partitioned Parquet file saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "AZ7BgY61sRqY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- bathrooms: string (nullable = true)\n",
      " |-- sqft_living: integer (nullable = true)\n",
      " |-- sqft_lot: string (nullable = true)\n",
      " |-- floors: integer (nullable = true)\n",
      " |-- waterfront: string (nullable = true)\n",
      " |-- view: integer (nullable = true)\n",
      " |-- sale_date: date (nullable = true)\n",
      " |-- date_built: integer (nullable = true)\n",
      "\n",
      "+--------------------+----------+--------+--------+---------+-----------+--------+------+----------+----+----------+----------+\n",
      "|                  id|      date|   price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view| sale_date|date_built|\n",
      "+--------------------+----------+--------+--------+---------+-----------+--------+------+----------+----+----------+----------+\n",
      "|2ed8d509-7372-46d...|2021-08-06|258710.0|       3|        3|       1918|    9666|     1|         0|  25|2021-08-06|      2015|\n",
      "|941bad30-eb49-4a7...|2020-05-09|229896.0|       3|        3|       2197|    8641|     1|         0|   3|2020-05-09|      2015|\n",
      "|c797ca12-52cd-4b1...|2019-06-08|288650.0|       2|        3|       2100|   10419|     2|         0|   7|2019-06-08|      2015|\n",
      "|0cfe57f3-28c2-472...|2019-10-04|308313.0|       3|        3|       1960|    9453|     2|         0|   2|2019-10-04|      2015|\n",
      "|d715f295-2fbf-4e9...|2021-05-17|391574.0|       3|        2|       1635|    8040|     2|         0|  10|2021-05-17|      2015|\n",
      "+--------------------+----------+--------+--------+---------+-----------+--------+------+----------+----+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11. Read the formatted parquet data.\n",
    "# Define the path to the partitioned Parquet data\n",
    "parquet_path = \"home_sales_partitioned\"\n",
    "\n",
    "# Read the Parquet file\n",
    "df_parquet = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Show the schema to verify it's correctly loaded\n",
    "df_parquet.printSchema()\n",
    "\n",
    "# Display a sample of the data\n",
    "df_parquet.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|date_built|\n",
      "+----------+\n",
      "|      2015|\n",
      "|      2017|\n",
      "|      2012|\n",
      "|      2014|\n",
      "|      2011|\n",
      "|      2016|\n",
      "|      2010|\n",
      "|      2013|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the distinct years when homes were built and display them  \n",
    "df_parquet.select(\"date_built\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+--------+---------+-----------+--------+------+----------+----+----------+----------+\n",
      "|                  id|      date|   price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view| sale_date|date_built|\n",
      "+--------------------+----------+--------+--------+---------+-----------+--------+------+----------+----+----------+----------+\n",
      "|f8a53099-ba1c-47d...|2022-04-08|936923.0|       4|        3|       3167|   11733|     2|         1|  76|2022-04-08|      2016|\n",
      "|b672c137-b88c-48b...|2019-10-16|239895.0|       2|        2|       1631|   11149|     2|         0|   0|2019-10-16|      2016|\n",
      "|e81aacfe-17fe-46b...|2020-06-16|181925.0|       3|        3|       2137|   11709|     2|         0|  22|2020-06-16|      2016|\n",
      "|dd61eb34-6589-4c0...|2021-07-25|210247.0|       3|        2|       1672|   11986|     2|         0|  28|2021-07-25|      2016|\n",
      "|f233cb41-6f33-4b0...|2021-07-18|437375.0|       4|        3|       1704|   11721|     2|         0|  34|2021-07-18|      2016|\n",
      "|4566cd2a-ac6e-435...|2019-07-15|177541.0|       3|        3|       2130|   10517|     2|         0|  25|2019-07-15|      2016|\n",
      "|e0fc52aa-c349-4ba...|2019-04-17|202790.0|       3|        3|       2025|   10987|     2|         0|  19|2019-04-17|      2016|\n",
      "|2dc662fe-57b8-44b...|2020-04-20|417215.0|       4|        2|       2104|    8227|     2|         0|  48|2020-04-20|      2016|\n",
      "|d5f2062a-8950-417...|2019-09-02|305980.0|       4|        2|       1799|   13004|     1|         0|  26|2019-09-02|      2016|\n",
      "|513c8016-c85b-4f6...|2019-04-13|138006.0|       4|        2|       1780|   14211|     2|         0|  12|2019-04-13|      2016|\n",
      "|08784e80-3b51-483...|2020-04-28|273544.0|       2|        3|       1664|   10137|     2|         0|  22|2020-04-28|      2016|\n",
      "|77a2d239-67a3-43b...|2021-06-18|396508.0|       3|        3|       1890|   10081|     2|         0|  25|2021-06-18|      2016|\n",
      "|f470d9ea-5edb-4e2...|2019-05-14|441656.0|       3|        2|       2434|   14563|     2|         0|  36|2019-05-14|      2016|\n",
      "|7c8448c9-d6a7-46a...|2021-01-25|137482.0|       2|        2|       2353|   11308|     2|         0|  49|2021-01-25|      2016|\n",
      "|7acb88a1-8a4f-4a7...|2019-03-31|351363.0|       3|        3|       1771|   13037|     2|         0|  13|2019-03-31|      2016|\n",
      "|01c9ef98-40a7-4f8...|2019-07-27|322051.0|       4|        2|       2195|   13510|     1|         0|  13|2019-07-27|      2016|\n",
      "|ede93538-bd44-434...|2021-03-02|283444.0|       4|        3|       1942|    8083|     2|         0|  36|2021-03-02|      2016|\n",
      "|00ce46df-952c-4ce...|2020-05-11|430471.0|       3|        3|       1935|   13334|     2|         0|  10|2020-05-11|      2016|\n",
      "|63aefaa5-a03b-4c0...|2020-04-28|196895.0|       2|        3|       2043|   13617|     2|         0|  34|2020-04-28|      2016|\n",
      "|c4495471-5dc3-42d...|2021-08-19|294828.0|       4|        2|       2215|    9393|     2|         0|  46|2021-08-19|      2016|\n",
      "+--------------------+----------+--------+--------+---------+-----------+--------+------+----------+----+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data from the Parquet file located at 'parquet_path'\n",
    "# and filter the DataFrame to only include rows where 'date_built' is 2016.\n",
    "df_2000 = spark.read.parquet(parquet_path).filter(\"date_built == 2016\")\n",
    "\n",
    "# Display the contents of the filtered DataFrame.\n",
    "df_2000.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "J6MJkHfvVcvh"
   },
   "outputs": [],
   "source": [
    "# 12. Create a temporary table for the parquet data.\n",
    "# Define the path to the partitioned Parquet data\n",
    "parquet_path = \"home_sales_partitioned\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df_parquet = spark.read.parquet(parquet_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import round, avg\n",
    "\n",
    "# Define the Parquet file path\n",
    "parquet_path = \"home_sales_partitioned\"\n",
    "\n",
    "# Read the Parquet file\n",
    "df_parquet = spark.read.parquet(parquet_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_Vhb52rU1Sn",
    "outputId": "a0b8d0c4-55ed-4c6c-bfd8-4c8c5334838e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|view| avg_price|\n",
      "+----+----------+\n",
      "| 100| 1026669.5|\n",
      "|  99|1061201.42|\n",
      "|  98|1053739.33|\n",
      "|  97|1129040.15|\n",
      "|  96|1017815.92|\n",
      "|  95| 1054325.6|\n",
      "|  94| 1033536.2|\n",
      "|  93|1026006.06|\n",
      "|  92| 970402.55|\n",
      "|  91|1137372.73|\n",
      "|  90|1062654.16|\n",
      "|  89|1107839.15|\n",
      "|  88|1031719.35|\n",
      "|  87| 1072285.2|\n",
      "|  86|1070444.25|\n",
      "|  85|1056336.74|\n",
      "|  84|1117233.13|\n",
      "|  83|1033965.93|\n",
      "|  82| 1063498.0|\n",
      "|  81|1053472.79|\n",
      "+----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "--- 5.0861029624938965 seconds (Parquet) ---\n"
     ]
    }
   ],
   "source": [
    "# 13. Using the parquet DataFrame, run the last query above, that calculates\n",
    "# the average price of a home per \"view\" rating, rounded to two decimal places,\n",
    "# having an average home price greater than or equal to $350,000.\n",
    "# Determine the runtime and compare it to the cached runtime.\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the query using DataFrame operations\n",
    "parquet_result = (\n",
    "    df_parquet.groupBy(\"view\")\n",
    "    .agg(round(avg(\"price\"), 2).alias(\"avg_price\"))\n",
    "    .filter(\"avg_price >= 350000\")\n",
    "    .orderBy(\"view\", ascending=False)\n",
    ")\n",
    "\n",
    "# Show results\n",
    "parquet_result.show()\n",
    "\n",
    "# End timer and print execution time\n",
    "print(\"--- %s seconds (Parquet) ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in directory: ['._SUCCESS.crc', 'date_built=2010', 'date_built=2011', 'date_built=2012', 'date_built=2013', 'date_built=2014', 'date_built=2015', 'date_built=2016', 'date_built=2017', '_SUCCESS']\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the directory containing the partitioned Parquet files\n",
    "parquet_path = \"home_sales_partitioned\"\n",
    "\n",
    "# List and print all files in the specified directory\n",
    "print(\"Files in directory:\", os.listdir(parquet_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+--------+---------+-----------+--------+------+----------+----+----------+----------+\n",
      "|                  id|      date|   price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view| sale_date|date_built|\n",
      "+--------------------+----------+--------+--------+---------+-----------+--------+------+----------+----+----------+----------+\n",
      "|2ed8d509-7372-46d...|2021-08-06|258710.0|       3|        3|       1918|    9666|     1|         0|  25|2021-08-06|      2015|\n",
      "|941bad30-eb49-4a7...|2020-05-09|229896.0|       3|        3|       2197|    8641|     1|         0|   3|2020-05-09|      2015|\n",
      "|c797ca12-52cd-4b1...|2019-06-08|288650.0|       2|        3|       2100|   10419|     2|         0|   7|2019-06-08|      2015|\n",
      "|0cfe57f3-28c2-472...|2019-10-04|308313.0|       3|        3|       1960|    9453|     2|         0|   2|2019-10-04|      2015|\n",
      "|d715f295-2fbf-4e9...|2021-05-17|391574.0|       3|        2|       1635|    8040|     2|         0|  10|2021-05-17|      2015|\n",
      "+--------------------+----------+--------+--------+---------+-----------+--------+------+----------+----+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the partitioned Parquet files into a Spark DataFrame  \n",
    "df_parquet = spark.read.parquet(\"home_sales_partitioned\")  \n",
    "\n",
    "# Display the first 5 rows to verify that the data is loaded correctly  \n",
    "df_parquet.show(5)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or replace a temporary SQL table named \"home_sales_parquet\"  \n",
    "# This allows querying the DataFrame using SQL queries within Spark  \n",
    "df_parquet.createOrReplaceTempView(\"home_sales_parquet\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='home_data', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='home_sales', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='home_sales_parquet', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all the tables currently registered in the Spark catalog\n",
    "# This will return information about all temporary and permanent tables available for querying\n",
    "spark.catalog.listTables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hjjYzQGjtbq8",
    "outputId": "830549fd-bb41-451b-9183-5ebf6e3e470b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is home_sales_parquet cached? False\n"
     ]
    }
   ],
   "source": [
    "# 14. Uncache the home_sales temporary table.\n",
    "# Uncache the temporary table\n",
    "spark.catalog.uncacheTable(\"home_sales_parquet\")\n",
    "\n",
    "# Verify if the table is uncached\n",
    "print(\"Is home_sales_parquet cached?\", spark.catalog.isCached(\"home_sales_parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sy9NBvO7tlmm",
    "outputId": "be73e0e3-5e85-4794-aad9-025fb6fa84a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is home_sales_parquet cached? False\n"
     ]
    }
   ],
   "source": [
    "# 15. Check if the home_sales is no longer cached\n",
    "# Verify if the table is still cached\n",
    "print(\"Is home_sales_parquet cached?\", spark.catalog.isCached(\"home_sales_parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Home_Sales_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
